# Homework 3: GPT-OSS-20B Transformer Architecture

This repository contains the implementation and experiments for **Homework 3**, focused on exploring the architecture of a modern decoder-only Transformer, specifically the **GPT-OSS-20B** model.

## Overview

During class discussions, we explored Transformer blocks and their significance in modern language models. In this homework, the goal was to examine and implement key components of a **GPT-style decoder-only transformer**. This includes:

* Understanding the inner workings of transformer blocks.
* Implementing the architecture of **GPT-OSS-20B**.
* Training the model using the **language modeling objective**.

## Results

* Achieved **90% accuracy** on the assigned task.
* Added multiple **plots** to visualize training progress, model performance, and attention patterns.

## Files

* `hw_3_gpt_oss_student.ipynb` – Main notebook containing implementation, experiments, and plots.
* `README.md` – This file with an overview of the project.

## Key Features

* Step-by-step implementation of a GPT-style transformer block.
* Training using a language modeling objective.
* Visualization of performance metrics and attention mechanisms.
